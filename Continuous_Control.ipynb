{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "This notebook uses the [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/) environment to train a DDPG model for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893)._\n",
    "\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "If the code cell below returns an error, please double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and all the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#import os\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = UnityEnvironment(file_name='env/Reacher20')\n",
    "env = UnityEnvironment(file_name='env/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. State and Action Spaces\n",
    "\n",
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents.\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of the agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_env(actions):\n",
    "    \n",
    "    env_info = env.step(actions)[brain_name]    # sends the action of each agent for the environment\n",
    "    next_states = env_info.vector_observations  # get the next state for each agent\n",
    "    rewards =  env_info.rewards                 # get the reward for each agent\n",
    "    dones = env_info.local_done                 # check if any episode has finished\n",
    "    \n",
    "    return next_states, rewards, dones, env_info\n",
    "\n",
    "def ddpg(agent, n_episodes=2000, max_t=1000):\n",
    "    \n",
    "    scores = []                        # list with the scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 10 scores\n",
    "    scores_window_mean =  []           # the mean of the last 10 episodes\n",
    "    scores_window_std  =  []           # std of the last 10 episodes\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        agent.reset()\n",
    "        states = env_info.vector_observations             # get the current state for each agent\n",
    "        score = 0.0                                       # initialize the score for each agent\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states)                     # selects an action\n",
    "            next_states, rewards, dones,_ = step_env(actions) # sends the action and obtain next_states, rewards, \n",
    "                                                            # dones for each agent\n",
    "            \n",
    "            agent.step(states, actions, rewards, next_states, dones) # execute the action\n",
    "            \n",
    "            states = next_states      # update the current state\n",
    "            score += np.mean(rewards) # update the score\n",
    "            \n",
    "            if np.any(dones):\n",
    "                break\n",
    "            \n",
    "        scores.append(score)                           # save most recent score\n",
    "        scores_window.append(score)                    \n",
    "        w_score_mean = np.mean(scores_window)\n",
    "        scores_window_mean.append(w_score_mean)\n",
    "        scores_window_std.append(np.std(scores_window))\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f} \\tScore: {:.2f}'.format(i_episode, w_score_mean, score), end=\"\")\n",
    "        \n",
    "        if i_episode % 10 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f} \\tScore: {:.2f}'.format(i_episode, w_score_mean, score))     \n",
    "        \n",
    "        if w_score_mean >= 30:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, w_score_mean))\n",
    "            \n",
    "            filename = \"checkpoints/{}_checkpoint_{}.pth\"\n",
    "            torch.save(agent.actor_local.state_dict(),  filename.format(type(agent).__name__, \"actor\"))\n",
    "            torch.save(agent.critic_local.state_dict(), filename.format(type(agent).__name__, \"critic\"))\n",
    "            break\n",
    "            \n",
    "    return (scores, scores_window_mean, scores_window_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg.agent import DDPGAgent\n",
    "agent = DDPGAgent(state_size, action_size, num_agents, seed=42, update_frequency=20, n_learns=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the architecture of the Actor and Critic network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.actor_local)\n",
    "print(agent.critic_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an DDPG agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, scores_window_mean, scores_window_std = ddpg(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores, rolling_mean, rolling_std, title, filename=''):\n",
    "    \n",
    "    \"\"\"Plot the scores with the moving average\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    ax.axhline(y=30., xmin=0.0, xmax=1.0, color='r', linestyle='--', alpha=0.9, label=\"Score threshold baseline\")\n",
    "    \n",
    "    plt.plot(np.arange(len(scores)), scores);\n",
    "    plt.plot(np.arange(len(rolling_mean)), rolling_mean,label='Moving average');\n",
    "    plt.fill_between(np.arange(len(rolling_mean)), rolling_mean + rolling_std, rolling_mean - rolling_std, \n",
    "                     facecolor='green', alpha=0.4, label=\"Std of the moving average\");\n",
    "    plt.ylabel(\"Score\"); plt.xlabel(\"Episode #\"); plt.title(title);\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_xlim(0, len(scores) + 1)\n",
    "    \n",
    "    if filename != '':\n",
    "        fig.savefig(filename, format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(scores, np.array(scores_window_mean), np.array(scores_window_std), \n",
    "            title='DDPG Learning Curve', filename='report/figures/{}_score.pdf'.format(type(agent).__name__));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Watch a smart agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor_local.load_state_dict(torch.load('checkpoints/{}_checkpoint_actor.pth'.format(type(agent).__name__)))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint/{}_checkpoint_critic.pth'.format(type(agent).__name__)))\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state for each agent\n",
    "scores = np.zeros(num_agents)                          # initialize the score for each agent\n",
    "\n",
    "while True:\n",
    "    actions = agent.act(states)                        # select actions from loaded model agent\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "    next_states = env_info.vector_observations         # get the next state for each agent\n",
    "    rewards = env_info.rewards                         # get the reward for each agent\n",
    "    dones = env_info.local_done                        # see if episode has finished\n",
    "    scores += env_info.rewards                         # update the score for each agent\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
